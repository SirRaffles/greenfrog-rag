version: '3.8'

services:
  # ChromaDB Vector Database
  chromadb:
    image: chromadb/chroma:0.5.20
    container_name: greenfrog-chromadb
    ports:
      - "${CHROMADB_PORT:-8001}:8000"
    volumes:
      - ./data/chromadb:/chroma/chroma
    environment:
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_HTTP_PORT=8000
      - ALLOW_RESET=true
    restart: unless-stopped
    networks:
      - greenfrog-network
    # healthcheck disabled - ChromaDB container doesn't have python/curl/wget
    # We'll rely on backend service to check connectivity
    healthcheck:
      test: ["CMD-SHELL", "exit 0"]
      interval: 30s
      timeout: 10s
      retries: 1
      start_period: 10s

  # Redis for semantic caching
  redis:
    image: redis:7-alpine
    container_name: greenfrog-redis
    ports:
      - "${REDIS_PORT:-6400}:6379"
    volumes:
      - ./data/redis:/data
    command: redis-server --save 60 1 --loglevel warning
    restart: unless-stopped
    networks:
      - greenfrog-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  # AnythingLLM with RAG capabilities
  anythingllm:
    image: mintplexlabs/anythingllm:latest
    container_name: greenfrog-anythingllm
    ports:
      - "${ANYTHINGLLM_PORT:-3001}:3001"
    volumes:
      - ./data/anythingllm:/app/server/storage
      - ./anythingllm/env.txt:/app/server/.env
    environment:
      - STORAGE_DIR=/app/server/storage
      - VECTOR_DB=chroma
      - CHROMA_ENDPOINT=http://chromadb:8000
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - OLLAMA_MODEL_PREF=llama3.2:3b
      - OLLAMA_RESPONSE_TIMEOUT=30000
      - OLLAMA_KEEP_ALIVE_TIMEOUT=300
      - EMBEDDING_ENGINE=ollama
      - EMBEDDING_MODEL_PREF=nomic-embed-text:latest
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - chromadb
    restart: unless-stopped
    networks:
      - greenfrog-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/api/ping"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # Piper TTS - Primary Text-to-Speech (Fast, CPU-optimized)
  piper-tts:
    build:
      context: ./piper-tts
      dockerfile: Dockerfile
    container_name: greenfrog-piper
    ports:
      - "${PIPER_TTS_PORT:-5000}:5000"
    volumes:
      - ./piper-tts/models:/models
      - ./piper-tts/cache:/cache
    environment:
      - PIPER_VOICE=${PIPER_VOICE_MODEL:-en_US-lessac-medium}
      - PIPER_RATE=${TTS_SPEAKING_RATE:-1.0}
    restart: unless-stopped
    networks:
      - greenfrog-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # XTTS-v2 - Optional Voice Cloning (Slower but supports cloning)
  xtts:
    build:
      context: ./xtts-v2
      dockerfile: Dockerfile
    container_name: greenfrog-xtts
    ports:
      - "${XTTS_PORT:-5001}:5000"
    volumes:
      - ./xtts-v2/models:/models
      - ./xtts-v2/voices:/voices
      - ./xtts-v2/outputs:/outputs
    environment:
      - COQUI_TOS_AGREED=1
      - PYTHONUNBUFFERED=1
      - XTTS_DEVICE=${XTTS_DEVICE:-cpu}
      - XTTS_LANGUAGE=${XTTS_LANGUAGE:-en}
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 3G
    restart: unless-stopped
    networks:
      - greenfrog-network
    profiles:
      - optional
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # SadTalker - Avatar Generation
  sadtalker:
    image: wawa9000/sadtalker:latest
    container_name: greenfrog-sadtalker
    ports:
      - "${SADTALKER_PORT:-7860}:7860"
    volumes:
      - ./sadtalker/checkpoints:/app/checkpoints
      - ./sadtalker/gfpgan:/app/gfpgan
      - ./sadtalker/inputs:/app/inputs
      - ./sadtalker/results:/app/results
    environment:
      - PYTHONUNBUFFERED=1
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
    restart: unless-stopped
    networks:
      - greenfrog-network
    shm_size: 2gb

  # Website Scraper
  scraper:
    build:
      context: ./scraper
      dockerfile: Dockerfile
    container_name: greenfrog-scraper
    volumes:
      - ./scraper:/app
      - ./data/scraped:/data
      - ./logs/scraper:/logs
    environment:
      - WEBSITE_URL=https://www.thematchainitiative.com
      - ANYTHINGLLM_API=http://anythingllm:3001
      - SCRAPE_SCHEDULE=0 2 * * *
      - PYTHONUNBUFFERED=1
    depends_on:
      anythingllm:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - greenfrog-network

  # FastAPI Backend - Orchestration Layer
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: greenfrog-backend
    ports:
      - "${BACKEND_PORT:-8000}:8000"
    volumes:
      - ./backend:/app
      - ./logs/backend:/logs
      - ./data:/app/data
    environment:
      - ANYTHINGLLM_URL=http://anythingllm:3001
      - ANYTHINGLLM_API_KEY=sk-voAkPDg71LdDD5EBajbwgXdQ0qmUWbScmzzhMJue9WA
      - OLLAMA_API=http://host.docker.internal:11434
      - PIPER_TTS_API=http://piper-tts:5000
      - XTTS_API=http://xtts:5000
      - SADTALKER_API=http://sadtalker:7860
      - OLLAMA_MODEL=llama3.2:3b
      - TTS_MODE=${TTS_MODE:-piper}
      - CORS_ORIGINS=http://localhost:3000,http://192.168.50.171:3000,http://greenfrog.v4value.ai
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # RAG V2 Configuration
      - USE_RAG_V2=true
      - USE_CACHE=true
      - USE_RERANK=true
      - REDIS_URL=redis://greenfrog-redis:6379
      - CHROMADB_URL=http://chromadb:8000
      - CHROMADB_COLLECTION=greenfrog
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - OLLAMA_TIMEOUT=300
      - CACHE_SIMILARITY_THRESHOLD=0.95
      - CACHE_TTL_SECONDS=3600
      - RERANK_MODEL=score-based
      - EMBEDDING_MODEL=nomic-embed-text:latest
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      anythingllm:
        condition: service_healthy
      piper-tts:
        condition: service_healthy
      redis:
        condition: service_healthy
      chromadb:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - greenfrog-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Next.js Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: greenfrog-frontend
    ports:
      - "${FRONTEND_PORT:-3000}:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://192.168.50.171:8000
      - NEXT_PUBLIC_WS_URL=ws://192.168.50.171:8000/ws
      - NODE_ENV=production
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - greenfrog-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  greenfrog-network:
    driver: bridge
    name: greenfrog-network

volumes:
  chromadb-data:
  anythingllm-data:
  scraped-data:
  logs:
